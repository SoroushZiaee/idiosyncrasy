{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r /home/soroush1/projects/def-kohitij/soroush1/idiosyncrasy/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script_dir = '/lustre06/project/6067616/soroush1/idiosyncrasy/notebooks'\n",
      "parent_dir = '/lustre06/project/6067616/soroush1/idiosyncrasy'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "script_dir = os.path.dirname(os.getcwd())  # Get the directory where the script is located\n",
    "parent_dir = os.path.dirname(script_dir)  # Get the parent directory\n",
    "\n",
    "print(f\"{script_dir = }\")\n",
    "print(f\"{parent_dir = }\")\n",
    "\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_modules.datamodule import MuriDataModule\n",
    "from argparse import Namespace\n",
    "from datasets.Muri import MuriDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Namespace(\n",
    "    data_dir=\"/scratch/soroush1/memorability/muri1320\",\n",
    "    image_size=224,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    change_labels = False,\n",
    "    pin_memories=[False, False, False],  # [train, val, test]\n",
    "    return_paths = True\n",
    ")\n",
    "\n",
    "# Create the DataModule\n",
    "data_module = MuriDataModule(hparams)\n",
    "\n",
    "# Prepare data and setup\n",
    "data_module.prepare_data()\n",
    "data_module.setup(\"test\")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"test dataset size: {len(data_module.test_dataset)}\")\n",
    "\n",
    "test_dl = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.Muri import MuriDataset\n",
    "\n",
    "ds = MuriDataset(root=\"/scratch/soroush1/memorability/muri1320\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "import PIL.Image\n",
    "import re\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MuriDataset(Dataset):\n",
    "    def __init__(self, root: str, transforms=None, return_paths=False):\n",
    "\n",
    "        self.root = root\n",
    "\n",
    "        self.transforms = transforms\n",
    "        # Sort image files\n",
    "        self.meta_data = pd.read_csv(os.path.join(root, \"meta.csv\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.meta_data[\"img_path\"][idx]\n",
    "        print(f\"{img_path = }\")\n",
    "        image = PIL.Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        # Get the object label\n",
    "        label = self.meta_data[\"labels\"][idx]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def val_transform(input_size: int = 256):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MuriDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mMuriDataset\u001b[49m(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scratch/soroush1/memorability/muri1320\u001b[39m\u001b[38;5;124m\"\u001b[39m, transforms\u001b[38;5;241m=\u001b[39mval_transform(input_size\u001b[38;5;241m=\u001b[39minput_size))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ds)\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m x, y \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MuriDataset' is not defined"
     ]
    }
   ],
   "source": [
    "input_size = 256\n",
    "ds = MuriDataset(root=\"/scratch/soroush1/memorability/muri1320\", transforms=val_transform(input_size=input_size))\n",
    "print(f\"{len(ds) = }\")\n",
    "x, y = ds[0]\n",
    "x.size(), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/scratch/soroush1/memorability/muri1320/meta.csv\")\n",
    "\n",
    "# How many of the labels are in the dataset?\n",
    "# use groupby to count the number of unique labels\n",
    "# then print the dataframe\n",
    "\n",
    "df.groupby(\"labels\").size().reset_index(name=\"counts\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.5, random_state=42, stratify=df[\"labels\"]) \n",
    "\n",
    "# save index of train_df and test_df\n",
    "train_df[\"Unnamed: 0\"].to_csv(\"/scratch/soroush1/memorability/muri1320/train.csv\", index=False)\n",
    "test_df[\"Unnamed: 0\"].to_csv(\"/scratch/soroush1/memorability/muri1320/test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
