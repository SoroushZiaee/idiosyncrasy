{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c14660-ad5b-4748-9c55-c994463ceaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-index h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47bde1e1-d774-405d-9afc-622af02eb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f41e6c-3123-4b7b-9f64-40e64b8100d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script_dir = '/lustre06/project/6067616/soroush1/idiosyncrasy/notebooks'\n",
      "parent_dir = '/lustre06/project/6067616/soroush1/idiosyncrasy'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "script_dir = os.path.dirname(os.getcwd())  # Get the directory where the script is located\n",
    "parent_dir = os.path.dirname(script_dir)  # Get the parent directory\n",
    "\n",
    "print(f\"{script_dir = }\")\n",
    "print(f\"{parent_dir = }\")\n",
    "\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb4a6df-73e4-43af-a3f1-1e8de85d59c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.models.feature_extraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resnet50\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_graph_node_names\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.models.feature_extraction'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.feature_extraction import get_graph_node_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8035ca90-2f94-44b5-af3a-080bf88373ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision.models.feature_extraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlit_modules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferioTemporalLayer\n",
      "File \u001b[0;32m/lustre06/project/6067616/soroush1/idiosyncrasy/lit_modules/modules/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_lightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelLightning\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwm_model_lightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResNetLSTMModule\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelLightning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNetLSTMModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/lustre06/project/6067616/soroush1/idiosyncrasy/lit_modules/modules/model_lightning.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models, transforms\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     create_feature_extractor,\n\u001b[1;32m     13\u001b[0m     get_graph_node_names,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MeanSquaredError, Accuracy\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision.models.feature_extraction'"
     ]
    }
   ],
   "source": [
    "from lit_modules.datamodule import MuriDataModule\n",
    "from argparse import Namespace\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50\n",
    "import torch as ch\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from lit_modules.modules.utils import InferioTemporalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9a29e-be03-48fc-9d66-789bb37bfd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "hparams = Namespace(\n",
    "    data_dir=\"/scratch/soroush1/memorability/muri1320\",\n",
    "    image_size=224,\n",
    "    batch_size=128,\n",
    "    num_workers=4,\n",
    "    change_labels = False,\n",
    "    pin_memories=[False, False, False],  # [train, val, test]\n",
    "    return_paths = True\n",
    ")\n",
    "\n",
    "# Create the DataModule\n",
    "data_module = MuriDataModule(hparams)\n",
    "\n",
    "# Prepare data and setup\n",
    "data_module.prepare_data()\n",
    "data_module.setup(\"test\")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"test dataset size: {len(data_module.test_dataset)}\")\n",
    "\n",
    "test_dl = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb615c0-2d82-436b-bb2d-53ac5380fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, img_path in tqdm(test_dl):\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        # logger.info(f\"Using input size: {img.size()}\")\n",
    "        output = model(img)[getattr(InferioTemporalLayer, arch.upper()).value]  # extract IT layer\n",
    "        output, img_path = sort_batch_by_filename(output, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bd801-f7b6-4659-8b84-1d0e5d842435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlurPoolConv2d(ch.nn.Module):\n",
    "\n",
    "    # Purpose: This class creates a convolutional layer that first applies a blurring filter to the input before performing the convolution operation.\n",
    "    # Condition: The function apply_blurpool iterates over all layers of the model and replaces convolution layers (ch.nn.Conv2d) with BlurPoolConv2d if they have a stride greater than 1 and at least 16 input channels.\n",
    "    # Preventing Aliasing: Blurring the output of convolution layers (especially those with strides greater than 1) helps to reduce aliasing effects. Aliasing occurs when high-frequency signals are sampled too sparsely, leading to incorrect representations.\n",
    "    # Smooth Transitions: Applying a blur before downsampling ensures that transitions between pixels are smooth, preserving important information in the feature maps.\n",
    "    # Stabilizing Training: Blurring can help stabilize training by reducing high-frequency noise, making the model less sensitive to small changes in the input data.\n",
    "    def __init__(self, conv):\n",
    "        super().__init__()\n",
    "        default_filter = ch.tensor([[[[1, 2, 1], [2, 4, 2], [1, 2, 1]]]]) / 16.0\n",
    "        filt = default_filter.repeat(conv.in_channels, 1, 1, 1)\n",
    "        self.conv = conv\n",
    "        self.register_buffer(\"blur_filter\", filt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blurred = F.conv2d(\n",
    "            x,\n",
    "            self.blur_filter,\n",
    "            stride=1,\n",
    "            padding=(1, 1),\n",
    "            groups=self.conv.in_channels,\n",
    "            bias=None,\n",
    "        )\n",
    "        return self.conv.forward(blurred)\n",
    "\n",
    "\n",
    "def remove_prefix(state_dict, prefix):\n",
    "    \"\"\"\n",
    "    Remove a prefix from the state_dict keys.\n",
    "\n",
    "    Args:\n",
    "    state_dict (dict): State dictionary from which the prefix will be removed.\n",
    "    prefix (str): Prefix to be removed.\n",
    "\n",
    "    Returns:\n",
    "    dict: State dictionary with prefix removed from keys.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        key[len(prefix) :]: value\n",
    "        for key, value in state_dict.items()\n",
    "        if key.startswith(prefix)\n",
    "    }\n",
    "\n",
    "\n",
    "def match_and_load_weights(checkpoint_state_dict, model, prefix=\"module.\"):\n",
    "    \"\"\"\n",
    "    Match weights from checkpoint_state_dict with model's state_dict and load them into the model.\n",
    "\n",
    "    Args:\n",
    "    checkpoint_state_dict (dict): State dictionary from checkpoint.\n",
    "    model (torch.nn.Module): The model instance.\n",
    "    prefix (str): Prefix to be removed from checkpoint keys.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Remove the prefix from checkpoint state dict keys\n",
    "    cleaned_checkpoint_state_dict = remove_prefix(checkpoint_state_dict, prefix)\n",
    "\n",
    "    model_state_dict = model.state_dict()\n",
    "    matched_weights = {}\n",
    "\n",
    "    # Iterate over the cleaned checkpoint state dict\n",
    "    for ckpt_key, ckpt_weight in cleaned_checkpoint_state_dict.items():\n",
    "        if ckpt_key in model_state_dict:\n",
    "            # If the layer name matches, add to the matched_weights dict\n",
    "            matched_weights[ckpt_key] = ckpt_weight\n",
    "        else:\n",
    "            print(\n",
    "                f\"Layer {ckpt_key} from checkpoint not found in the model state dict.\"\n",
    "            )\n",
    "\n",
    "    return matched_weights\n",
    "\n",
    "def create_model_and_scaler(config):\n",
    "    arch = config[\"arch\"]\n",
    "    weights = config[\"weights\"]\n",
    "    use_blurpool = config[\"use_blurpool\"]\n",
    "    device = ch.device(\"cuda\" if ch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"{device = }\")\n",
    "\n",
    "    model = getattr(models, arch)(pretrained=None)\n",
    "    \n",
    "\n",
    "    def apply_blurpool(mod: ch.nn.Module):\n",
    "        for name, child in mod.named_children():\n",
    "            if isinstance(child, ch.nn.Conv2d) and (\n",
    "                np.max(child.stride) > 1 and child.in_channels >= 16\n",
    "            ):\n",
    "                setattr(mod, name, BlurPoolConv2d(child))\n",
    "            else:\n",
    "                apply_blurpool(child)\n",
    "\n",
    "    if use_blurpool:\n",
    "        apply_blurpool(model)\n",
    "\n",
    "    ckpt = ch.load(weights,weights_only=True, map_location=device)\n",
    "    print(f\"{list(ckpt.keys())[:10] = }\")\n",
    "    ckpt = match_and_load_weights(ckpt, model)\n",
    "    print(f\"{list(ckpt.keys())[:10] = }\")\n",
    "    \n",
    "    \n",
    "    model = model.to(device)    \n",
    "    model.load_state_dict(ckpt)\n",
    "    \n",
    "    return model\n",
    "\n",
    "config = {\"weights\": \"weights/experiment_different_initialization/resnet50-0/final_weights.pt\", \"arch\": \"resnet50\", \"use_blurpool\": True, }\n",
    "model = create_model_and_scaler(config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9fe6f-7ddf-4865-9ee4-bdb5e17edce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_batch_by_filename(tensor, filenames):\n",
    "    # Extract indices from filenames\n",
    "    indices = [int(re.search(r'(\\d+)', fname).group()) for fname in filenames]\n",
    "    \n",
    "    # Create a list of (index, tensor_slice, filename) tuples\n",
    "    indexed_data = list(zip(indices, tensor, filenames))\n",
    "    \n",
    "    # Sort the list based on the extracted indices\n",
    "    sorted_data = sorted(indexed_data, key=lambda x: x[0])\n",
    "    \n",
    "    # Unzip the sorted list\n",
    "    _, sorted_tensor, sorted_filenames = zip(*sorted_data)\n",
    "    \n",
    "    # Stack the tensor slices back into a single tensor\n",
    "    sorted_tensor = torch.stack(sorted_tensor)\n",
    "    \n",
    "    return sorted_tensor, sorted_filenames\n",
    "\n",
    "def extract_and_concatenate_features(model, test_dl, arch):\n",
    "    all_outputs = []\n",
    "    all_img_paths = []\n",
    "    for img, img_path in tqdm(test_dl):\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            # logger.info(f\"Using input size: {img.size()}\")\n",
    "            output = model(img)[getattr(InferioTemporalLayer, arch.upper()).value]  # extract IT layer\n",
    "            output, img_path = sort_batch_by_filename(output, img_path)\n",
    "        \n",
    "        all_outputs.append(output.cpu())  # Move to CPU to save GPU memory\n",
    "        all_img_paths.extend(img_path)\n",
    "        \n",
    "        # Delete batch data to free up memory\n",
    "        del img, output\n",
    "        torch.cuda.empty_cache()  # Clear CUDA cache\n",
    "    \n",
    "    # Concatenate all outputs into a single tensor\n",
    "    concatenated_output = torch.cat(all_outputs, dim=0)\n",
    "    \n",
    "    # Delete all_outputs to free up memory\n",
    "    del all_outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Reshape the output to (samples, -1)\n",
    "    reshaped_output = concatenated_output.view(concatenated_output.size(0), -1)\n",
    "    \n",
    "    # Convert to numpy and delete the torch tensor\n",
    "    reshaped_output = reshaped_output.detach().numpy()\n",
    "    del concatenated_output\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return reshaped_output, all_img_paths\n",
    "\n",
    "def save_h5(data, model_name, task_name, dst_path: str, layer_name: str = \"it\", ):\n",
    "    \"\"\"\n",
    "    Load features from a pickle file and save them to an HDF5 file.\n",
    "\n",
    "    Args:\n",
    "    model_name (str): Name of the model used to extract features.\n",
    "    task_name (str): Name of the task or dataset.\n",
    "    layer_name (str): Name of the layer from which features were extracted.\n",
    "    dst_path (str): Directory to save the HDF5 file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "        f\"Model: {model_name}, Task: {task_name}, Layer: {layer_name}, Shape: {data.shape}\"\n",
    "    )\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(dst_path, exist_ok=True)\n",
    "\n",
    "    # Construct the full path for the HDF5 file\n",
    "    h5_file = os.path.join(dst_path, f\"{model_name}_{task_name}_1.h5\")\n",
    "\n",
    "    # Save the NumPy array as a .h5 file\n",
    "    with h5py.File(h5_file, \"w\") as hf:\n",
    "        hf.create_dataset(\"features\", data=data)\n",
    "\n",
    "    print(f\"Features saved to {h5_file}\")\n",
    "\n",
    "\n",
    "def get_config(model_name: str, checkpoint: str):\n",
    "    num_classes = 1000\n",
    "    task_type = \"classification\"\n",
    "    return {\n",
    "    \"arch\": model_name,\n",
    "    \"use_blurpool\": True,\n",
    "    \"pretrained\": False,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"lr\": 0.0001,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"momentum\": 0.9,\n",
    "    \"nesterov\": True,\n",
    "    \"norm_mean\": [0.485, 0.456, 0.406],\n",
    "    \"norm_std\": [0.229, 0.224, 0.225],\n",
    "    \"task_type\": task_type,\n",
    "    \"experiment\": \"one\",\n",
    "    \"optimizer\": \"sgd\",\n",
    "    \"scheduler\": \"plateau\",\n",
    "    \"step_size\": 30,\n",
    "    \"max_epochs\": 100,\n",
    "    \"random_training\": False,\n",
    "    \"use_ckpt\": True,\n",
    "    \"checkpoint\": checkpoint,\n",
    "    }\n",
    "\n",
    "def get_data(input_size: int):\n",
    "    # Define hyperparameters\n",
    "    hparams = Namespace(\n",
    "        data_dir=\"/scratch/soroush1/memorability/muri1320\",\n",
    "        image_size=input_size,\n",
    "        batch_size=128,\n",
    "        num_workers=4,\n",
    "        change_labels = False,\n",
    "        pin_memories=[False, False, False],  # [train, val, test]\n",
    "        return_paths = True\n",
    "    )\n",
    "    \n",
    "    # Create the DataModule\n",
    "    data_module = MuriDataModule(hparams)\n",
    "    \n",
    "    # Prepare data and setup\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup(\"test\")\n",
    "    \n",
    "    # Print dataset sizes\n",
    "    print(f\"test dataset size: {len(data_module.test_dataset)}\")\n",
    "    \n",
    "    return data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7a029-692c-4687-9868-f45da4b300d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract IT activation\n",
    "model_name = \"alexnet\"\n",
    "\n",
    "concatenated_features, all_image_paths = extract_and_concatenate_features(model, test_dl, model_name)\n",
    "\n",
    "print(f\"Concatenated features shape: {concatenated_features.shape}\")\n",
    "print(f\"Total number of images: {len(all_image_paths)}\")\n",
    "\n",
    "task_name = \"imagenet_shuffle\"\n",
    "dst_dir = \"./features\"\n",
    "\n",
    "# Call the save_h5 function\n",
    "save_h5(concatenated_features, model_name, task_name, dst_dir)\n",
    "\n",
    "# Load the saved data and compare\n",
    "expected_file = os.path.join(dst_dir, f\"{model_name}_{task_name}_1.h5\")\n",
    "with h5py.File(expected_file, \"r\") as hf:\n",
    "    saved_data = hf[\"features\"][:]\n",
    "\n",
    "saved_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
