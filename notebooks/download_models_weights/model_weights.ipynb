{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load pretrained weights for resnet50: get_model() takes 1 positional argument but 2 were given\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 3, 224, 224]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 213\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ModularBackboneLSTM(\n\u001b[1;32m    203\u001b[0m         backbone_type\u001b[38;5;241m=\u001b[39mbackbone_type,\n\u001b[1;32m    204\u001b[0m         num_classes\u001b[38;5;241m=\u001b[39mnum_classes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m         freeze_backbone\u001b[38;5;241m=\u001b[39mfreeze_backbone\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Example usage   \u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRESNET50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    217\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 202\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(backbone_name, num_classes, hidden_size, num_layers, dropout_rate, pretrained, freeze_backbone)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported backbone: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackbone_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported backbones: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[b\u001b[38;5;241m.\u001b[39mname\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mb\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mBackboneType]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModularBackboneLSTM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackbone_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_backbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_backbone\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mModularBackboneLSTM.__init__\u001b[0;34m(self, backbone_type, num_classes, hidden_size, num_layers, dropout_rate, pretrained, freeze_backbone)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_layer \u001b[38;5;241m=\u001b[39m FeatureExtractorLayer[backbone_type\u001b[38;5;241m.\u001b[39mname]\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Initialize backbone and get its output features\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_backbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze_backbone\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(dropout_rate)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# LSTM layer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 131\u001b[0m, in \u001b[0;36mModularBackboneLSTM._initialize_backbone\u001b[0;34m(self, backbone_type, pretrained, freeze_backbone)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    130\u001b[0m         dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m) \u001b[38;5;66;03m# (batch_size, timesteps, channels, height, width)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m modified_backbone, out_features\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torchvision/models/resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    144\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 146\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:455\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    453\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    454\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 1, 3, 224, 224]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from enum import Enum\n",
    "from typing import Optional, Dict, Any\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BackboneType(Enum):\n",
    "    ALEXNET = (\"alexnet\", \"IMAGENET1K_V1\")\n",
    "    RESNET18 = (\"resnet18\", \"IMAGENET1K_V1\")\n",
    "    RESNET50 = (\"resnet50\", \"IMAGENET1K_V1\")\n",
    "    RESNET101 = (\"resnet101\", \"IMAGENET1K_V1\")\n",
    "    VGG16 = (\"vgg16\", \"IMAGENET1K_V1\")\n",
    "    VGG19 = (\"vgg19\", \"IMAGENET1K_V1\")\n",
    "    INCEPTION_V3 = (\"inception_v3\", \"IMAGENET1K_V1\")\n",
    "    VIT_B_16 = (\"vit_b_16\", \"IMAGENET1K_V1\")\n",
    "    VIT_B_32 = (\"vit_b_32\", \"IMAGENET1K_V1\")\n",
    "    EFFICIENTNET_B0 = (\"efficientnet_b0\", \"IMAGENET1K_V1\")\n",
    "\n",
    "    def __init__(self, model_name: str, weights_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.weights_name = weights_name\n",
    "\n",
    "class FeatureExtractorLayer(Enum):\n",
    "    ALEXNET = \"features.12\"\n",
    "    RESNET18 = \"layer4.0.relu\"\n",
    "    RESNET50 = \"layer3.2.bn1\"\n",
    "    RESNET101 = \"layer3.2.bn1\"\n",
    "    VGG16 = \"features.30\"\n",
    "    VGG19 = \"features.36\"\n",
    "    INCEPTION_V3 = \"Mixed_7a.branch3x3_1.bn\"\n",
    "    VIT_B_16 = \"encoder.layers.encoder_layer_8.mlp\"\n",
    "    VIT_B_32 = \"encoder.layers.encoder_layer_8.mlp\"\n",
    "    EFFICIENTNET_B0 = \"features.6.2.stochastic_depth\"\n",
    "\n",
    "class ModularBackboneLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_type: BackboneType,\n",
    "        num_classes: int,\n",
    "        hidden_size: int = 256,\n",
    "        num_layers: int = 1,\n",
    "        dropout_rate: float = 0.5,\n",
    "        pretrained: bool = True,\n",
    "        freeze_backbone: bool = True\n",
    "    ):\n",
    "        super(ModularBackboneLSTM, self).__init__()\n",
    "        print(f\"{backbone_type = }\")\n",
    "        self.backbone_type = backbone_type\n",
    "        self.feature_layer = FeatureExtractorLayer[backbone_type.name].value\n",
    "        \n",
    "        # Initialize backbone and get its output features\n",
    "        self.backbone, self.backbone_features = self._initialize_backbone(\n",
    "            backbone_type, pretrained, freeze_backbone\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.backbone_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feature combination layer\n",
    "        self.combine_features = nn.Sequential(\n",
    "            nn.Linear(self.backbone_features + hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _initialize_backbone(\n",
    "        self, \n",
    "        backbone_type: BackboneType, \n",
    "        pretrained: bool,\n",
    "        freeze_backbone: bool\n",
    "    ) -> tuple[nn.Module, int]:\n",
    "        \"\"\"Initialize the backbone model and return it along with its output features size.\"\"\"\n",
    "        \n",
    "        # Get the model creation function\n",
    "        model_func = getattr(models, backbone_type.model_name)\n",
    "        \n",
    "        # Initialize the model with or without pretrained weights\n",
    "        if pretrained:\n",
    "            try:\n",
    "                # Get weights using models.get_weight\n",
    "                # weights = models.get_weight(backbone_type.model_name, weights)\n",
    "                # print(f\"{weights = }\")\n",
    "                # model = model_func(weights=weights)\n",
    "                model = models.get_model(backbone_type.model_name, backbone_type.weights_name)\n",
    "                logger.info(f\"Loaded pretrained weights: {backbone_type.weights_name}\")\n",
    "                print(f\"{model = }\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load pretrained weights for {backbone_type.model_name}: {e}\")\n",
    "                model = model_func(weights=None)\n",
    "        else:\n",
    "            model = model_func(weights=None)\n",
    "            \n",
    "        # Get the feature extraction layer\n",
    "        feature_layer = FeatureExtractorLayer[backbone_type.name].value\n",
    "        \n",
    "        # Create feature extractor\n",
    "        layers = []\n",
    "        current_layer = model\n",
    "        \n",
    "        for part in feature_layer.split('.'):\n",
    "            current_layer = getattr(current_layer, part)\n",
    "            layers.append(current_layer)\n",
    "            \n",
    "        modified_backbone = nn.Sequential(*layers)\n",
    "        \n",
    "        # Freeze backbone if specified\n",
    "        if freeze_backbone:\n",
    "            for param in modified_backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # Get output features size\n",
    "        if backbone_type in [BackboneType.VIT_B_16, BackboneType.VIT_B_32]:\n",
    "            out_features = 768  # Standard ViT hidden size\n",
    "        else:\n",
    "            # Use a forward pass with dummy data to get output size\n",
    "            with torch.no_grad():\n",
    "                dummy_input = torch.randn(1, 1, 3, 224, 224) # (batch_size, timesteps, channels, height, width)\n",
    "                out = modified_backbone(dummy_input)\n",
    "                out_features = out.view(1, -1).size(1)\n",
    "        \n",
    "        return modified_backbone, out_features\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        \n",
    "        # Process each timestep through backbone\n",
    "        backbone_features = []\n",
    "        for t in range(timesteps):\n",
    "            current_frame = x[:, t, :, :, :]\n",
    "            features = self.backbone(current_frame)\n",
    "            features = features.view(batch_size, -1)\n",
    "            backbone_features.append(features.unsqueeze(1))\n",
    "        \n",
    "        # Combine backbone features\n",
    "        backbone_features = torch.cat(backbone_features, dim=1)\n",
    "        backbone_features = self.dropout(backbone_features)\n",
    "        \n",
    "        # Process through LSTM\n",
    "        lstm_out, (h_n, c_n) = self.lstm(backbone_features)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Get final outputs\n",
    "        lstm_last_out = lstm_out[:, -1, :]\n",
    "        backbone_last_out = backbone_features[:, -1, :]\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat((backbone_last_out, lstm_last_out), dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # Final processing\n",
    "        combined_features = self.combine_features(combined)\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(combined_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def create_model(\n",
    "    backbone_name: str,\n",
    "    num_classes: int,\n",
    "    hidden_size: int = 256,\n",
    "    num_layers: int = 1,\n",
    "    dropout_rate: float = 0.5,\n",
    "    pretrained: bool = True,\n",
    "    freeze_backbone: bool = True\n",
    ") -> ModularBackboneLSTM:\n",
    "    \"\"\"\n",
    "    Create a ModularBackboneLSTM model with the specified backbone.\n",
    "    \n",
    "    Args:\n",
    "        backbone_name: Name of the backbone architecture\n",
    "        num_classes: Number of output classes\n",
    "        hidden_size: LSTM hidden size\n",
    "        num_layers: Number of LSTM layers\n",
    "        dropout_rate: Dropout rate\n",
    "        pretrained: Whether to use pretrained weights\n",
    "        freeze_backbone: Whether to freeze backbone parameters\n",
    "    \n",
    "    Returns:\n",
    "        ModularBackboneLSTM model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        backbone_type = BackboneType[backbone_name.upper()]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Unsupported backbone: {backbone_name}. \"\n",
    "                        f\"Supported backbones: {[b.name for b in BackboneType]}\")\n",
    "    \n",
    "    return ModularBackboneLSTM(\n",
    "        backbone_type=backbone_type,\n",
    "        num_classes=num_classes,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout_rate=dropout_rate,\n",
    "        pretrained=pretrained,\n",
    "        freeze_backbone=freeze_backbone\n",
    "    )\n",
    "\n",
    "# Example usage   \n",
    "model = create_model(\n",
    "    backbone_name=\"RESNET50\",\n",
    "    num_classes=10,\n",
    "    pretrained=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-24 20:47:36,203 - INFO - Using device: cpu\n",
      "2024-10-24 20:47:36,204 - INFO - Starting backbone tests...\n",
      "2024-10-24 20:47:36,416 - INFO - \n",
      "Testing ALEXNET...\n",
      "2024-10-24 20:47:36,417 - WARNING - No pretrained weights found for alexnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soroush1/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/soroush1/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-24 20:47:51,775 - INFO - ✓ ALEXNET passed (Time: 15.36s)\n",
      "2024-10-24 20:47:51,788 - INFO - \n",
      "Testing RESNET18...\n",
      "2024-10-24 20:47:51,788 - WARNING - No pretrained weights found for resnet18\n",
      "2024-10-24 20:47:52,182 - ERROR - ✗ RESNET18 failed: Given groups=1, weight of size [512, 256, 3, 3], expected input[1, 3, 224, 224] to have 256 channels, but got 3 channels instead\n",
      "2024-10-24 20:47:52,188 - INFO - \n",
      "Testing RESNET50...\n",
      "2024-10-24 20:47:52,188 - WARNING - No pretrained weights found for resnet50\n",
      "2024-10-24 20:47:52,434 - ERROR - ✗ RESNET50 failed: Given groups=1, weight of size [256, 512, 1, 1], expected input[1, 3, 224, 224] to have 512 channels, but got 3 channels instead\n",
      "2024-10-24 20:47:52,446 - INFO - \n",
      "Testing RESNET101...\n",
      "2024-10-24 20:47:52,447 - WARNING - No pretrained weights found for resnet101\n",
      "2024-10-24 20:47:53,027 - ERROR - ✗ RESNET101 failed: Given groups=1, weight of size [256, 512, 1, 1], expected input[1, 3, 224, 224] to have 512 channels, but got 3 channels instead\n",
      "2024-10-24 20:47:53,046 - INFO - \n",
      "Testing VGG16...\n",
      "2024-10-24 20:47:53,046 - WARNING - No pretrained weights found for vgg16\n",
      "2024-10-24 20:48:30,367 - INFO - ✓ VGG16 passed (Time: 37.32s)\n",
      "2024-10-24 20:48:30,383 - INFO - \n",
      "Testing VGG19...\n",
      "2024-10-24 20:48:30,383 - WARNING - No pretrained weights found for vgg19\n",
      "2024-10-24 20:49:14,659 - INFO - ✓ VGG19 passed (Time: 44.28s)\n",
      "2024-10-24 20:49:14,746 - INFO - \n",
      "Testing INCEPTION_V3...\n",
      "2024-10-24 20:49:14,750 - WARNING - No pretrained weights found for inception_v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soroush1/projects/def-kohitij/soroush1/WM_age_of_ultron/.venv/lib/python3.11/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-24 20:49:59,900 - ERROR - ✗ INCEPTION_V3 failed: Given groups=1, weight of size [192, 768, 1, 1], expected input[1, 3, 224, 224] to have 768 channels, but got 3 channels instead\n",
      "2024-10-24 20:49:59,914 - INFO - \n",
      "Testing VIT_B_16...\n",
      "2024-10-24 20:49:59,915 - WARNING - No pretrained weights found for vit_b_16\n",
      "2024-10-24 20:50:01,165 - ERROR - ✗ VIT_B_16 failed: Expected (batch_size, seq_length, hidden_dim) got torch.Size([2, 3, 224, 224])\n",
      "2024-10-24 20:50:01,191 - INFO - \n",
      "Testing VIT_B_32...\n",
      "2024-10-24 20:50:01,192 - WARNING - No pretrained weights found for vit_b_32\n",
      "2024-10-24 20:50:02,354 - ERROR - ✗ VIT_B_32 failed: Expected (batch_size, seq_length, hidden_dim) got torch.Size([2, 3, 224, 224])\n",
      "2024-10-24 20:50:02,446 - INFO - \n",
      "Testing EFFICIENTNET_B0...\n",
      "2024-10-24 20:50:02,448 - WARNING - No pretrained weights found for efficientnet_b0\n",
      "2024-10-24 20:50:47,847 - ERROR - ✗ EFFICIENTNET_B0 failed: Given groups=1, weight of size [672, 112, 1, 1], expected input[1, 1280, 7, 7] to have 112 channels, but got 1280 channels instead\n",
      "2024-10-24 20:50:47,850 - INFO - \n",
      "Test Summary\n",
      "2024-10-24 20:50:47,850 - INFO - ==================================================\n",
      "2024-10-24 20:50:47,851 - INFO - Model                Status    \n",
      "2024-10-24 20:50:47,851 - INFO - --------------------------------------------------\n",
      "2024-10-24 20:50:47,851 - INFO - ALEXNET              ✓ PASSED  \n",
      "2024-10-24 20:50:47,852 - INFO - RESNET18             ✗ FAILED  \n",
      "2024-10-24 20:50:47,852 - INFO - RESNET50             ✗ FAILED  \n",
      "2024-10-24 20:50:47,852 - INFO - RESNET101            ✗ FAILED  \n",
      "2024-10-24 20:50:47,853 - INFO - VGG16                ✓ PASSED  \n",
      "2024-10-24 20:50:47,853 - INFO - VGG19                ✓ PASSED  \n",
      "2024-10-24 20:50:47,853 - INFO - INCEPTION_V3         ✗ FAILED  \n",
      "2024-10-24 20:50:47,854 - INFO - VIT_B_16             ✗ FAILED  \n",
      "2024-10-24 20:50:47,854 - INFO - VIT_B_32             ✗ FAILED  \n",
      "2024-10-24 20:50:47,854 - INFO - EFFICIENTNET_B0      ✗ FAILED  \n",
      "2024-10-24 20:50:47,855 - INFO - ==================================================\n",
      "2024-10-24 20:50:47,855 - INFO - Total Passed: 3/10\n",
      "2024-10-24 20:50:47,855 - INFO - Total Time: 191.65s\n",
      "2024-10-24 20:50:47,856 - INFO - ==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "import sys\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    stream=sys.stdout\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def test_backbone(\n",
    "    backbone_name: str,\n",
    "    model_config: Dict[str, Any],\n",
    "    input_size: Dict[str, int]\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Test a single backbone model.\n",
    "    \n",
    "    Args:\n",
    "        backbone_name: Name of the backbone to test\n",
    "        model_config: Configuration for the model\n",
    "        input_size: Input tensor dimensions\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if test passed, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create sample input\n",
    "        sample_input = torch.randn(\n",
    "            input_size['batch_size'],\n",
    "            input_size['timesteps'],\n",
    "            input_size['channels'],\n",
    "            input_size['height'],\n",
    "            input_size['width']\n",
    "        )\n",
    "        \n",
    "        # Create and evaluate model\n",
    "        logger.info(f\"\\nTesting {backbone_name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model = create_model(backbone_name=backbone_name, **model_config)\n",
    "        model.eval()\n",
    "        \n",
    "        # Test forward pass\n",
    "        with torch.no_grad():\n",
    "            output = model(sample_input)\n",
    "        \n",
    "        # Verify output\n",
    "        expected_shape = (input_size['batch_size'], model_config['num_classes'])\n",
    "        assert tuple(output.shape) == expected_shape, \\\n",
    "            f\"Wrong output shape: {tuple(output.shape)} vs {expected_shape}\"\n",
    "        \n",
    "        assert not torch.isnan(output).any(), \"Output contains NaN values\"\n",
    "        assert not torch.isinf(output).any(), \"Output contains Inf values\"\n",
    "        \n",
    "        end_time = time.time()\n",
    "        logger.info(f\"✓ {backbone_name} passed (Time: {end_time - start_time:.2f}s)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ {backbone_name} failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main test function to test all backbones.\"\"\"\n",
    "    logger.info(\"Starting backbone tests...\")\n",
    "    \n",
    "    # Default model configuration\n",
    "    model_config = {\n",
    "        'num_classes': 10,\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 1,\n",
    "        'dropout_rate': 0.5,\n",
    "        'pretrained': True,\n",
    "        'freeze_backbone': True\n",
    "    }\n",
    "    \n",
    "    # Default input sizes\n",
    "    default_input_size = {\n",
    "        'batch_size': 2,\n",
    "        'timesteps': 3,\n",
    "        'channels': 3,\n",
    "        'height': 224,\n",
    "        'width': 224\n",
    "    }\n",
    "    \n",
    "    # Special configurations for specific models\n",
    "    special_configs = {\n",
    "        'INCEPTION_V3': {\n",
    "            'height': 299,\n",
    "            'width': 299\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # List all available backbones\n",
    "    backbones = [b.name for b in BackboneType]\n",
    "    \n",
    "    # Test results\n",
    "    results = {}\n",
    "    start_time_total = time.time()\n",
    "    \n",
    "    # Test each backbone\n",
    "    for backbone_name in backbones:\n",
    "        # Update input size if needed\n",
    "        input_size = default_input_size.copy()\n",
    "        if backbone_name in special_configs:\n",
    "            input_size.update(special_configs[backbone_name])\n",
    "        \n",
    "        # Run test\n",
    "        results[backbone_name] = test_backbone(\n",
    "            backbone_name=backbone_name,\n",
    "            model_config=model_config,\n",
    "            input_size=input_size\n",
    "        )\n",
    "    \n",
    "    # Print summary\n",
    "    end_time_total = time.time()\n",
    "    total_time = end_time_total - start_time_total\n",
    "    \n",
    "    logger.info(\"\\nTest Summary\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"{'Model':<20} {'Status':<10}\")\n",
    "    logger.info(\"-\" * 50)\n",
    "    \n",
    "    passed = 0\n",
    "    for backbone, status in results.items():\n",
    "        passed += int(status)\n",
    "        status_str = \"✓ PASSED\" if status else \"✗ FAILED\"\n",
    "        logger.info(f\"{backbone:<20} {status_str:<10}\")\n",
    "    \n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"Total Passed: {passed}/{len(results)}\")\n",
    "    logger.info(f\"Total Time: {total_time:.2f}s\")\n",
    "    logger.info(\"=\" * 50)\n",
    "    \n",
    "    # Additional info about memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(\"\\nGPU Memory Usage\")\n",
    "        logger.info(\"-\" * 50)\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            memory_allocated = torch.cuda.memory_allocated(i) / 1024**2\n",
    "            memory_reserved = torch.cuda.memory_reserved(i) / 1024**2\n",
    "            logger.info(f\"GPU {i}:\")\n",
    "            logger.info(f\"  Allocated: {memory_allocated:.2f} MB\")\n",
    "            logger.info(f\"  Reserved:  {memory_reserved:.2f} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check for CUDA\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Run tests\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
